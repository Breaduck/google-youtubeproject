"""
LTX-Video Image-to-Video Service
ì”¬ ì´ë¯¸ì§€ë¥¼ 8ì´ˆ AI ì˜ìƒìœ¼ë¡œ ë³€í™˜í•˜ëŠ” Modal ì„œë¹„ìŠ¤
"""

import modal
import io
import base64
from pathlib import Path

# ============================================================================
# 1. ì´ë¯¸ì§€ ì„¤ì •
# ============================================================================
image = (
    modal.Image.debian_slim(python_version="3.12")
    .pip_install(
        "accelerate==1.6.0",
        "diffusers==0.33.1",
        "huggingface-hub==0.36.0",
        "imageio==2.37.0",
        "imageio-ffmpeg==0.5.1",
        "sentencepiece==0.2.0",
        "torch==2.7.0",
        "transformers==4.51.3",
        "pillow",
        "requests",
    )
    .env({"HF_XET_HIGH_PERFORMANCE": "1"})
)

# ============================================================================
# 2. ë³¼ë¥¨ ì„¤ì •
# ============================================================================
MODEL_VOLUME_NAME = "ltx-model"
model_volume = modal.Volume.from_name(MODEL_VOLUME_NAME, create_if_missing=True)

MODEL_PATH = Path("/models")
image = image.env({"HF_HOME": str(MODEL_PATH)})

MINUTES = 60

# ============================================================================
# 3. Modal ì•±
# ============================================================================
app = modal.App("ltx-video-service")

# ============================================================================
# 4. LTX ë¹„ë””ì˜¤ ìƒì„± í´ë˜ìŠ¤
# ============================================================================
@app.cls(
    image=image,
    volumes={MODEL_PATH: model_volume},
    gpu="A10G",  # A10G: ë¹„ìš© íš¨ìœ¨ì , í•„ìš”ì‹œ H100ìœ¼ë¡œ ë³€ê²½
    timeout=10 * MINUTES,
    secrets=[modal.Secret.from_name("huggingface-secret")],
)
class LTX:
    @modal.enter()
    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ (ì»¨í…Œì´ë„ˆ ì‹œì‘ì‹œ 1íšŒ ì‹¤í–‰)"""
        import torch
        from diffusers import LTXImageToVideoPipeline

        print("ğŸ”§ LTX-Video ëª¨ë¸ ë¡œë“œ ì¤‘...")

        # Image-to-Video íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
        self.pipe = LTXImageToVideoPipeline.from_pretrained(
            "Lightricks/LTX-Video",
            torch_dtype=torch.bfloat16
        )
        self.pipe.to("cuda")

        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

    @modal.method()
    def generate(
        self,
        image_url: str,
        prompt: str = "natural movement, subtle expressions, gentle background motion",
        negative_prompt: str = "worst quality, inconsistent motion, blurry, jittery, distorted",
        width: int = 704,
        height: int = 480,
        num_frames: int = 161,  # 8ì´ˆ @ 24fps = 192 í”„ë ˆì„, í•˜ì§€ë§Œ 161ì´ ê¶Œì¥ê°’
        num_inference_steps: int = 30,  # ì†ë„ì™€ í’ˆì§ˆì˜ ê· í˜•
        guidance_scale: float = 3.0,
        seed: int = 42,
    ) -> bytes:
        """
        ì´ë¯¸ì§€ URLì„ ë°›ì•„ì„œ 8ì´ˆ ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            image_url: ì…ë ¥ ì´ë¯¸ì§€ URL
            prompt: ì›€ì§ì„ ì„¤ëª… í”„ë¡¬í”„íŠ¸
            negative_prompt: í”¼í•  ìš”ì†Œë“¤
            width: ë¹„ë””ì˜¤ ë„ˆë¹„ (704 ê¶Œì¥)
            height: ë¹„ë””ì˜¤ ë†’ì´ (480 ê¶Œì¥)
            num_frames: í”„ë ˆì„ ìˆ˜ (161 = ~6.7ì´ˆ)
            num_inference_steps: ìƒì„± ìŠ¤í… (30-50 ê¶Œì¥)
            guidance_scale: ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼
            seed: ëœë¤ ì‹œë“œ

        Returns:
            MP4 ë¹„ë””ì˜¤ ë°”ì´íŠ¸
        """
        import torch
        import requests
        from PIL import Image
        from diffusers.utils import export_to_video
        import tempfile

        print(f"ğŸ¬ ë¹„ë””ì˜¤ ìƒì„± ì‹œì‘")
        print(f"   ì´ë¯¸ì§€: {image_url[:50]}...")
        print(f"   í”„ë¡¬í”„íŠ¸: {prompt}")
        print(f"   í¬ê¸°: {width}x{height}, {num_frames} í”„ë ˆì„")

        # 1. ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        print("ğŸ“¥ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        response = requests.get(image_url, timeout=30)
        response.raise_for_status()
        input_image = Image.open(io.BytesIO(response.content)).convert("RGB")

        # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ (ê¶Œì¥ í¬ê¸°ì— ë§ì¶¤)
        input_image = input_image.resize((width, height))

        # 2. ë¹„ë””ì˜¤ ìƒì„±
        print("ğŸ¥ AI ë¹„ë””ì˜¤ ìƒì„± ì¤‘...")

        with torch.inference_mode():
            result = self.pipe(
                image=input_image,
                prompt=prompt,
                negative_prompt=negative_prompt,
                width=width,
                height=height,
                num_frames=num_frames,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                decode_timestep=0.03,  # LTX ê¶Œì¥ê°’
                decode_noise_scale=0.025,  # LTX ê¶Œì¥ê°’
                generator=torch.Generator("cuda").manual_seed(seed),
            )

        frames = result.frames[0]  # List of PIL Images

        # 3. MP4ë¡œ ì €ì¥
        print("ğŸ’¾ ë¹„ë””ì˜¤ ì €ì¥ ì¤‘...")
        with tempfile.NamedTemporaryFile(suffix=".mp4", delete=False) as tmp:
            temp_path = tmp.name

        # 24fpsë¡œ ì €ì¥ (161 í”„ë ˆì„ = ì•½ 6.7ì´ˆ)
        export_to_video(frames, temp_path, fps=24)

        # 4. ë°”ì´íŠ¸ë¡œ ì½ê¸°
        with open(temp_path, "rb") as f:
            video_bytes = f.read()

        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        import os
        os.unlink(temp_path)

        size_mb = len(video_bytes) / 1024 / 1024
        print(f"âœ… ë¹„ë””ì˜¤ ìƒì„± ì™„ë£Œ! í¬ê¸°: {size_mb:.2f}MB")

        return video_bytes


# ============================================================================
# 5. ì›¹ ì—”ë“œí¬ì¸íŠ¸ (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ í˜¸ì¶œ)
# ============================================================================
@app.function(image=image)
@modal.web_endpoint(method="POST")
def generate_video_endpoint(item: dict) -> dict:
    """
    REST API ì—”ë“œí¬ì¸íŠ¸

    Request Body:
    {
        "image_url": "https://...",
        "prompt": "subtle facial expressions, gentle movement",
        "width": 704,
        "height": 480,
        "num_frames": 161,
        "seed": 42
    }

    Response:
    {
        "status": "success",
        "video_base64": "...",
        "size_mb": 12.5
    }
    """
    print(f"ğŸ¬ API ìš”ì²­ ë°›ìŒ: {item.get('image_url', 'N/A')[:50]}...")

    # LTX ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ë¹„ë””ì˜¤ ìƒì„±
    ltx = LTX()

    video_bytes = ltx.generate.remote(
        image_url=item["image_url"],
        prompt=item.get("prompt", "natural movement, subtle expressions"),
        negative_prompt=item.get(
            "negative_prompt",
            "worst quality, inconsistent motion, blurry, jittery, distorted"
        ),
        width=item.get("width", 704),
        height=item.get("height", 480),
        num_frames=item.get("num_frames", 161),
        num_inference_steps=item.get("num_inference_steps", 30),
        guidance_scale=item.get("guidance_scale", 3.0),
        seed=item.get("seed", 42),
    )

    # Base64 ì¸ì½”ë”©
    video_base64 = base64.b64encode(video_bytes).decode()
    size_mb = len(video_bytes) / 1024 / 1024

    print(f"âœ… API ì‘ë‹µ ì „ì†¡: {size_mb:.2f}MB")

    return {
        "status": "success",
        "video_base64": video_base64,
        "size_mb": round(size_mb, 2),
    }


# ============================================================================
# 6. ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©
# ============================================================================
@app.local_entrypoint()
def main(
    image_url: str = "https://picsum.photos/704/480",
    prompt: str = "gentle movements, natural lighting",
):
    """
    ë¡œì»¬ í…ŒìŠ¤íŠ¸

    Usage:
        modal run main.py
        modal run main.py --image-url="https://..." --prompt="..."
    """
    print("ğŸš€ LTX-Video ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸")
    print(f"   ì´ë¯¸ì§€: {image_url}")
    print(f"   í”„ë¡¬í”„íŠ¸: {prompt}")

    ltx = LTX()

    video_bytes = ltx.generate.remote(
        image_url=image_url,
        prompt=prompt,
    )

    # ë¡œì»¬ì— ì €ì¥
    output_path = Path("/tmp/test_output.mp4")
    output_path.write_bytes(video_bytes)

    print(f"âœ… ì™„ë£Œ! ë¹„ë””ì˜¤ ì €ì¥ë¨: {output_path}")
    print(f"   í¬ê¸°: {len(video_bytes) / 1024 / 1024:.2f}MB")
